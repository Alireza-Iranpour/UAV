{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import uniform, randint, ttest_rel, ttest_ind\n",
    "\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import cufflinks as cf\n",
    "\n",
    "# from pandas_profiling import ProfileReport\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif \n",
    "from sklearn.feature_selection import SelectPercentile, VarianceThreshold, SelectFromModel, RFE\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import LeaveOneOut, LeaveOneGroupOut\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "#preprocessing:\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, normalize, scale, Normalizer, MinMaxScaler, FunctionTransformer\n",
    "\n",
    "# models:\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set before running\n",
    "\n",
    "Across = True                           # (True = Across, False = Within)\n",
    "CA = True                               # (True = Cognative Ability, False = Task Performance)\n",
    "\n",
    "run = True                             # whether to perform training and cross-validation\n",
    "tune = False                           # whether to perform hyper-parameter tuning\n",
    "n_rep = 1\n",
    "cognitive_ability = 'Meara'             # choices: ('Meara', 'BarChartLit', 'VerbalWM_longest')\n",
    "performance_measure = 'mmd_accuracy'    # choices: ('mmd_accuracy', 'mmd_task_time')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------\n",
    "# loading the datasets\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "if (Across): # if Across Tasks\n",
    "    \n",
    "    n_windows = 15\n",
    "    target = cognitive_ability\n",
    "    result_path = (\"Results/A_\" + str(target))\n",
    "\n",
    "    for window in range(1, n_windows + 1):\n",
    "        path = (\"Data/Data_Across_CA/\"+ str(target) + \"/Task_\" + str(window) + \".csv\")\n",
    "        datasets[window] = pd.read_csv(path)\n",
    "\n",
    "else: # if Within Task\n",
    "    \n",
    "    n_windows = 29\n",
    "    \n",
    "    if (CA): # if CA\n",
    "        \n",
    "        target = cognitive_ability\n",
    "        result_path = (\"Results/W_\" + str(target))\n",
    "        \n",
    "        for window in range(1, n_windows + 1):\n",
    "            path = (\"Data/Data_Within_CA/\"+ str(target) + \"/Cognitive_\" + str(window) + \".csv\")\n",
    "            datasets[window] = pd.read_csv(path)  \n",
    "            \n",
    "    else: # if TP\n",
    "        \n",
    "        target = performance_measure\n",
    "        result_path = (\"Results/W_\" + str(target))\n",
    "        \n",
    "        for window in range(1, n_windows + 1):\n",
    "            path = (\"Data/Data_Within_TP/\"+ str(target) + \"/Time_\" + str(window) + \".csv\")\n",
    "            datasets[window] = pd.read_csv(path)\n",
    "        \n",
    "#------------------------------------------------------------------------------------------------\n",
    "# display(df)\n",
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "def result(df):\n",
    "    df['mean_Overall'] = df.groupby(\n",
    "        ['Classifier', 'Window'])['Overall_Acc'].transform('mean')\n",
    "\n",
    "    idx = df.groupby(\n",
    "        ['Classifier'])['mean_Overall'].transform(max) == df['mean_Overall']\n",
    "\n",
    "    display(df[idx].groupby(['Classifier']).mean()[['Window', 'Overall_Acc', 'Low_Acc', 'High_Acc']])\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "def save(phase, df):\n",
    "    path = (result_path + \"_\" + phase + \".csv\")\n",
    "    df.to_csv(path, index=True)\n",
    "    \n",
    "def read(phase):\n",
    "    return pd.read_csv((result_path + \"_\" + phase + \".csv\"), index_col=0)\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "def plot_classifier(df, classifier):\n",
    "    data = df[df['Classifier']==classifier].loc[:, ['Overall_Acc', 'Low_Acc', 'High_Acc']]\n",
    "    fig = data.iplot(asFigure=True, xTitle='Window', yTitle='Accuracy', \n",
    "              title=classifier, legend='top', theme='white')\n",
    "    fig.show()\n",
    "    \n",
    "#--------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models:\n",
    "\n",
    "classifiers = {\n",
    "    'RF'  : RandomForestClassifier(n_jobs=-1),\n",
    "    'GB'  : GradientBoostingClassifier(),\n",
    "    'XGB' : XGBClassifier(n_jobs=-1),\n",
    "    'LR'  : LogisticRegression(),\n",
    "    'SVM' : LinearSVC(dual=False),\n",
    "    'KNN' : KNeighborsClassifier(n_jobs=-1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Pipeline([\n",
    "    ('vart', VarianceThreshold(threshold=0.0)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('minmax', MinMaxScaler())])\n",
    "\n",
    "feature_selection = Pipeline([\n",
    "    ('sp', SelectPercentile(percentile=40))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# baseline result\n",
    "df = datasets[1]\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# baseline score\n",
    "DC = DummyClassifier(strategy=\"most_frequent\")\n",
    "DC.fit(X, y)\n",
    "baseline = DC.score(X, y)\n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'Original'\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "columns = [\"CA/TP\", \"Classifier\", \"Window\", \"Repetition\", \"Overall_Acc\", \"Low_Acc\", \"High_Acc\"]\n",
    "original = pd.DataFrame(columns=columns)\n",
    "\n",
    "if (run):\n",
    "    \n",
    "    for name, estimator in classifiers.items():\n",
    "\n",
    "        print(name)\n",
    "        if name in ['RF', 'GB']:\n",
    "            repetitions = n_rep\n",
    "        else:\n",
    "            repetitions = 1    \n",
    "\n",
    "        for window in range(1, n_windows + 1):\n",
    "\n",
    "            for rep in range(repetitions):\n",
    "\n",
    "                print(window, end=\"\\r\", flush=True)\n",
    "\n",
    "                df = datasets[window]\n",
    "\n",
    "                pipeline = Pipeline([\n",
    "                    ('vart', VarianceThreshold(threshold=0.0)),\n",
    "                    ('estimator', estimator)])\n",
    "\n",
    "                # evaluation\n",
    "                if (Across):\n",
    "                    X = df.drop(columns=[target])\n",
    "                    y = df[target]\n",
    "                    y_pred  = cross_val_predict(pipeline, X, y, cv=loo, n_jobs=-1, verbose=0)\n",
    "                \n",
    "                else:\n",
    "                    groups = df[\"Uid\"]\n",
    "                    X = df.drop(columns=[target, \"Sc_id\", \"Uid\"])\n",
    "                    y = df[target]\n",
    "                    y_pred  = cross_val_predict(pipeline, X, y, cv=logo.split(X, y, groups=groups), n_jobs=-1, verbose=0)\n",
    "                    \n",
    "                report  = classification_report(y, y_pred, output_dict=True)\n",
    "\n",
    "                # results\n",
    "                row = {\"CA/TP\"       : target,\n",
    "                       \"Classifier\"  : name,\n",
    "                       \"Window\"      : window,\n",
    "                       \"Repetition\"  : rep,\n",
    "                       \"Overall_Acc\" : report['accuracy'],\n",
    "                       \"Low_Acc\"     : report['0']['recall'],\n",
    "                       \"High_Acc\"    : report['1']['recall']       \n",
    "                      }\n",
    "\n",
    "                original = original.append(row, ignore_index=True) \n",
    "\n",
    "    save(phase=phase, df=original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = read(phase=phase)\n",
    "\n",
    "    # display(df.groupby(['Classifier', 'Window']).mean())\n",
    "    # display(df.groupby(['Classifier', 'Window']).std())\n",
    "\n",
    "    result(df)\n",
    "    # plot_classifier(df, 'SVM')\n",
    "    \n",
    "except:\n",
    "    print(\"File not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing (PP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'PP'\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "columns = [\"CA/TP\", \"Classifier\", \"Window\", \"Repetition\", \"Overall_Acc\", \"Low_Acc\", \"High_Acc\"]\n",
    "pp = pd.DataFrame(columns=columns)\n",
    "\n",
    "if (run):\n",
    "    \n",
    "    for name, estimator in classifiers.items():\n",
    "\n",
    "        print(name)\n",
    "        if name in ['RF', 'GB']:\n",
    "            repetitions = n_rep\n",
    "        else:\n",
    "            repetitions = 1    \n",
    "\n",
    "        for window in range(1, n_windows + 1):\n",
    "\n",
    "            for rep in range(repetitions):\n",
    "\n",
    "                print(window, end=\"\\r\", flush=True)\n",
    "\n",
    "                df = datasets[window]\n",
    "\n",
    "                pipeline = Pipeline([\n",
    "                    ('PP', preprocessing),\n",
    "                    ('estimator', estimator)])\n",
    "\n",
    "                # evaluation\n",
    "                if (Across):\n",
    "                    X = df.drop(columns=[target])\n",
    "                    y = df[target]\n",
    "                    y_pred  = cross_val_predict(pipeline, X, y, cv=loo, n_jobs=-1, verbose=0)\n",
    "                \n",
    "                else:\n",
    "                    groups = df[\"Uid\"]\n",
    "                    X = df.drop(columns=[target, \"Sc_id\", \"Uid\"])\n",
    "                    y = df[target]\n",
    "                    y_pred  = cross_val_predict(pipeline, X, y, cv=logo.split(X, y, groups=groups), n_jobs=-1, verbose=0)\n",
    "                    \n",
    "                report  = classification_report(y, y_pred, output_dict=True)\n",
    "\n",
    "                # results\n",
    "                row = {\"CA/TP\"       : target,\n",
    "                       \"Classifier\"  : name,\n",
    "                       \"Window\"      : window,\n",
    "                       \"Repetition\"  : rep,\n",
    "                       \"Overall_Acc\" : report['accuracy'],\n",
    "                       \"Low_Acc\"     : report['0']['recall'],\n",
    "                       \"High_Acc\"    : report['1']['recall']       \n",
    "                      }\n",
    "\n",
    "                pp = pp.append(row, ignore_index=True) \n",
    "\n",
    "    save(phase=phase, df=pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = read(phase=phase)\n",
    "\n",
    "    # display(df.groupby(['Classifier', 'Window']).mean())\n",
    "    # display(df.groupby(['Classifier', 'Window']).std())\n",
    "\n",
    "    result(df)\n",
    "    # plot_classifier(df, 'SVM')\n",
    "    \n",
    "except:\n",
    "    print(\"File not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing + Feature Selection (PP + FS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'PP+FS'\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "columns = [\"CA/TP\", \"Classifier\", \"Window\", \"Repetition\", \"Overall_Acc\", \"Low_Acc\", \"High_Acc\"]\n",
    "ppfs = pd.DataFrame(columns=columns)\n",
    "\n",
    "if (run):\n",
    "    \n",
    "    for name, estimator in classifiers.items():\n",
    "\n",
    "        print(name)\n",
    "        if name in ['RF', 'GB']:\n",
    "            repetitions = n_rep\n",
    "        else:\n",
    "            repetitions = 1    \n",
    "\n",
    "        for window in range(1, n_windows + 1):\n",
    "\n",
    "            for rep in range(repetitions):\n",
    "\n",
    "                print(window, end=\"\\r\", flush=True)\n",
    "\n",
    "                df = datasets[window]\n",
    "\n",
    "                pipeline = Pipeline([\n",
    "                    ('PP', preprocessing),\n",
    "                    ('FS', feature_selection),\n",
    "                    ('estimator', estimator)])\n",
    "\n",
    "                # evaluation\n",
    "                if (Across):\n",
    "                    X = df.drop(columns=[target])\n",
    "                    y = df[target]\n",
    "                    y_pred  = cross_val_predict(pipeline, X, y, cv=loo, n_jobs=-1, verbose=0)\n",
    "                \n",
    "                else:\n",
    "                    groups = df[\"Uid\"]\n",
    "                    X = df.drop(columns=[target, \"Sc_id\", \"Uid\"])\n",
    "                    y = df[target]\n",
    "                    y_pred  = cross_val_predict(pipeline, X, y, cv=logo.split(X, y, groups=groups), n_jobs=-1, verbose=0)\n",
    "                    \n",
    "                report  = classification_report(y, y_pred, output_dict=True)\n",
    "\n",
    "                # results\n",
    "                row = {\"CA/TP\"       : target,\n",
    "                       \"Classifier\"  : name,\n",
    "                       \"Window\"      : window,\n",
    "                       \"Repetition\"  : rep,\n",
    "                       \"Overall_Acc\" : report['accuracy'],\n",
    "                       \"Low_Acc\"     : report['0']['recall'],\n",
    "                       \"High_Acc\"    : report['1']['recall']       \n",
    "                      }\n",
    "\n",
    "                ppfs = ppfs.append(row, ignore_index=True) \n",
    "\n",
    "    save(phase=phase, df=ppfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = read(phase=phase)\n",
    "\n",
    "    # display(df.groupby(['Classifier', 'Window']).mean())\n",
    "    # display(df.groupby(['Classifier', 'Window']).std())\n",
    "\n",
    "    result(df)\n",
    "    # plot_classifier(df, 'SVM')\n",
    "    \n",
    "except:\n",
    "    print(\"File not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter distributions\n",
    "\n",
    "RF_dist = {\n",
    "    'estimator__n_estimators'  : Integer(10, 500),\n",
    "    'estimator__max_depth'     : Integer(1, 15)}\n",
    "\n",
    "LR_dist = {\n",
    "    'estimator__C'             : Real(1e-6, 1e+6, prior='log-uniform'),\n",
    "    'estimator__penalty'       : Categorical(['l1', 'l2'])}\n",
    "\n",
    "\n",
    "SVM_dist = {\n",
    "    'estimator__C'             : Real(1e-6, 1e+6, prior='log-uniform')}\n",
    "\n",
    "GB_dist = {\n",
    "    'estimator__loss'          : Categorical(['deviance', 'exponential']),\n",
    "    'estimator__max_depth'     : Integer(1, 10),\n",
    "    'estimator__gamma'         : Real(0, 0.5),\n",
    "    'estimator__learning_rate' : Real(0.05, 0.30)}\n",
    "\n",
    "XGB_dist = {\n",
    "    'estimator__max_depth'     : Integer(1, 10), \n",
    "    'estimator__gamma'         : Real(0, 0.5),\n",
    "    'estimator__learning_rate' : Real(0.05, 0.30)}\n",
    "\n",
    "KNN_dist = {\n",
    "    'estimator__n_neighbors'   : Integer(1,10)}\n",
    "\n",
    "distributions = {\n",
    "    'RF'  : RF_dist,\n",
    "    'LR'  : LR_dist,\n",
    "    'SVM' : SVM_dist,\n",
    "    'GB'  : GB_dist,\n",
    "    'XGB' : XGB_dist,\n",
    "    'KNN' : KNN_dist,}\n",
    "\n",
    "#--------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'tuned'\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "repetitions = n_rep\n",
    "columns = [\"CA/TP\", \"Classifier\", \"Repetition\", \"Overall_Acc\", \"Low_Acc\", \"High_Acc\"]\n",
    "tuned = pd.DataFrame(columns=columns)\n",
    "\n",
    "if (tune):\n",
    "    for name, estimator in classifiers.items():\n",
    "\n",
    "        if (name == \"RF\"):\n",
    "\n",
    "            for rep in range(repetitions):\n",
    "\n",
    "                print(rep, end=\"\\r\", flush=True)\n",
    "\n",
    "                best_window = 1\n",
    "                df = datasets[best_window]\n",
    "\n",
    "                pipeline = Pipeline([\n",
    "                        ('PP', preprocessing),\n",
    "                        ('FS', feature_selection),\n",
    "                        ('estimator', estimator)])\n",
    "\n",
    "                parameters = distributions[name]\n",
    "                \n",
    "                # evaluation\n",
    "                inner_loop = BayesSearchCV(pipeline, parameters, n_iter=10, n_points=5, cv=10, refit=True, n_jobs=-1, verbose=0)\n",
    "                \n",
    "                if (Across):\n",
    "                    X = df.drop(columns=[target])\n",
    "                    y = df[target]\n",
    "                    y_pred  = cross_val_predict(inner_loop, X, y, cv=loo, n_jobs=-1, verbose=0)\n",
    "                \n",
    "                else:\n",
    "                    groups = df[\"Uid\"]\n",
    "                    X = df.drop(columns=[target, \"Sc_id\", \"Uid\"])\n",
    "                    y = df[target]\n",
    "                    y_pred  = cross_val_predict(inner_loop, X, y, cv=logo.split(X, y, groups=groups), n_jobs=-1, verbose=0)\n",
    "                    \n",
    "                report  = classification_report(y, y_pred, output_dict=True)\n",
    "\n",
    "                # results\n",
    "                row = {\"CA/TP\"       : target,\n",
    "                       \"Classifier\"  : name,\n",
    "                       \"Repetition\"  : rep,\n",
    "                       \"Overall_Acc\" : report['accuracy'],\n",
    "                       \"Low_Acc\"     : report['0']['recall'],\n",
    "                       \"High_Acc\"    : report['1']['recall']       \n",
    "                      }\n",
    "\n",
    "                tuned = tuned.append(row, ignore_index=True) \n",
    "\n",
    "    save(phase=phase, df=tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = read(phase=phase)\n",
    "    df\n",
    "except:\n",
    "    print(\"File not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features for (Aross_Meara) \n",
      "\tClassifier: RF \n",
      "\tWindow: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>labels_fixationrate</th>\n",
       "      <td>0.026535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Relevant.bars_stddevfixationduration</th>\n",
       "      <td>0.022477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Refs_proportiontime</th>\n",
       "      <td>0.020720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels_meanfixationduration</th>\n",
       "      <td>0.020270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text_longestfixation</th>\n",
       "      <td>0.019693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meansaccadeduration</th>\n",
       "      <td>0.018248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>endpupilsize</th>\n",
       "      <td>0.016816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Relevant.bars_meanpupilvelocity</th>\n",
       "      <td>0.016751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meanfixationduration</th>\n",
       "      <td>0.015649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Viz_meanpupilvelocity</th>\n",
       "      <td>0.015582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Importance\n",
       "labels_fixationrate                     0.026535\n",
       "Relevant.bars_stddevfixationduration    0.022477\n",
       "Refs_proportiontime                     0.020720\n",
       "labels_meanfixationduration             0.020270\n",
       "Text_longestfixation                    0.019693\n",
       "meansaccadeduration                     0.018248\n",
       "endpupilsize                            0.016816\n",
       "Relevant.bars_meanpupilvelocity         0.016751\n",
       "meanfixationduration                    0.015649\n",
       "Viz_meanpupilvelocity                   0.015582"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Across = True                           # (True = Across, False = Within)\n",
    "CA = True                               # (True = Cognative Ability, False = Task Performance)\n",
    "\n",
    "cognitive_ability = 'Meara'             # choices: ('Meara', 'BarChartLit', 'VerbalWM_longest')\n",
    "performance_measure = 'mmd_accuracy'    # choices: ('mmd_accuracy', 'mmd_task_time')\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "name = 'RF'       # choices = ('RF', 'GB', 'XGB, 'LR', 'SVM')\n",
    "best_window = 4\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "estimator = classifiers[name]\n",
    "pipeline = Pipeline([\n",
    "                    ('PP', preprocessing),\n",
    "                    ('estimator', estimator)])\n",
    "\n",
    "if (Across):\n",
    "    target = cognitive_ability\n",
    "    df = pd.read_csv(\"Data/Data_Across_CA/\"+ str(target) + \"/Task_\" + str(best_window) + \".csv\")\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "                \n",
    "else:\n",
    "    if (CA):\n",
    "        target = cognitive_ability\n",
    "        df = pd.read_csv(\"Data/Data_Within_CA/\"+ str(target) + \"/Cognitive_\" + str(best_window) + \".csv\")\n",
    "    else:\n",
    "        target = performance_measure\n",
    "        df = pd.read_csv(\"Data/Data_Within_TP/\"+ str(target) + \"/Time_\" + str(best_window) + \".csv\")\n",
    "        \n",
    "    X = df.drop(columns=[target, \"Sc_id\", \"Uid\"])\n",
    "    y = df[target]\n",
    "\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "if name in ['RF', 'GB', 'XGB']:\n",
    "    data = pipeline['estimator'].feature_importances_\n",
    "    index = X.columns[pipeline['PP']['vart'].get_support()]\n",
    "    importances = pd.DataFrame(data=data, index=index, columns=[\"Importance\"])\n",
    "    features = importances.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "elif name in ['LR', 'SVM']:\n",
    "    data = pipeline['estimator'].coef_.ravel()\n",
    "    index = X.columns[pipeline['PP']['vart'].get_support()]\n",
    "    coefficients = pd.DataFrame(data=data, index=index, columns=[\"coef\"])\n",
    "    coefficients[\"|coef|\"] = coefficients[\"coef\"].abs()\n",
    "    features = coefficients.sort_values(by=\"|coef|\", ascending=False)\n",
    "\n",
    "if (Across):\n",
    "    print(f\"Top features for (Aross_{cognitive_ability}) \\n\\tClassifier: {name} \\n\\tWindow: {best_window}\")\n",
    "else:\n",
    "    if (CA):\n",
    "        print(f\"Top features for (Within_{cognitive_ability}) \\n\\tClassifier: {name} \\n\\tWindow: {best_window}\")\n",
    "    else:\n",
    "        print(f\"Top features for (Within_{performance_measure}) \\n\\tClassifier: {name} \\n\\tWindow: {best_window}\")\n",
    "        \n",
    "display(features.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
